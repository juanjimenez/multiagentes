\documentclass[10pt,a4paper]{report}
\usepackage[utf8]{inputenc}
\usepackage[spanish]{babel}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx}
\begin{document}
\section{Notas en la obtención de las ecuaciones de Kriging}
\subsection{definiciones básicas}
Consideramos un campo continuo e invariante en el tiempo,
\begin{equation*}
\phi:x\in D \rightarrow \phi(x) \in \mathbb{R}: D \subset \mathbb{R}^m
\end{equation*}
Modelamos $\phi$ como un proceso gaussiano,
\begin{equation}
\Omega(x) = r(x)^T\beta+ Z(x),
\end{equation}
Donde $r$ es un vector de funciones de regresión conocidas,\\
$r(x) = (r_1(x),r_2(x)\cdots, r_k(x))^T$ y $\beta =(\beta_1,\beta_2,\cdots, \beta_k)$ es un vector de parámetros, también conocidos.
 
Z(x) es un proceso gaussiano de media zero, cuya función de covarianza modelamos como,
\begin{equation}\label{eq:cov}
\begin{split}
\text{cov}(Z(x),Z(x')) &= \sigma_z^2\xi(x,x')\\
\xi(x,x') &= \exp\left[-\sum_i^m\frac{1}{\theta_i}\vert x_i-x'_i\vert ^{q_i}\right]
\end{split}
\end{equation}
Donde $x_i$ es la componente $i$ del vector $x$.
Supongamos que tenemos un conjunto de $n$ medidas ruidosas, de nuestro campo $y = [y(x^{(1)}),y(x^{(2)}),\cdots, y(x^{(n))}]$. Nuestras medidas son una realización del proceso gaussiano,
\begin{equation}
Y(x) = R(x)\beta + \mathcal{Z}
\end{equation}
Donde,
\begin{equation}
R = \begin{pmatrix}
r_1(x^{(1)})&\cdots&r_k(x^{(1)})\\
\vdots &\ddots&\vdots\\
r_1(x^{(n)})&\cdots&r_k(x^{(n)})
\end{pmatrix}
\end{equation}
con $\mathcal{Z} = (Z(x^{(1)}),\cdots,Z(x^{(n)}))^T$

Supongamos que queremos construir un predictor lineal $Y = a_x^Ty$ a partir de las medidas. Es decir yo quiero inferir cual sería el valor de mi campo en un punto x, supuesto que conozco los valores en los puntos medidos. De alguna manera, lo que estoy definiendo es una media ponderada de los valores que he medido. 

Podemos definir entonces el error cuadrático medio a partir del valor esperado de la diferencia entre las medidas y el modelo gaussiano,

\begin{equation}\label{eq:E}
\begin{split}
E[(a_x^Ty-Y(x))^2] &= E[a_x^Tyy^Ta_x + Y(x)^2 - 2a_x^TyY(x)]=\\
&E[a_x^T(R\beta+\mathcal{Z})((R\beta+\mathcal{Z})^Ta_x] +\\
&E[(r^T(x)\beta+Z(x))^2] +\\
&E[-2a_x^T(R\beta+\mathcal{Z})(r^T(x)\beta+Z(x))]
\end{split}
\end{equation}

Desarrollamos por separado cada uno de los tres sumandos de la ecuación (\ref{eq:E}),

\begin{enumerate}
\item \begin{equation}
\begin{split}
E[a_x^T(R\beta+\mathcal{Z})((R\beta+\mathcal{Z})^Ta_x] =\\
a_x^TR\beta\beta^TR^Ta_x +2a_x^TR\beta E[\mathcal{Z}^T]a_x + a_x^TE[\mathcal{Z}\mathcal{Z}^T]a_x=\\
=a_x^TR\beta\beta^TR^Ta_x +a_x^T\sigma_z^2Ka_x
\end{split}
\end{equation}
Donde hemos hecho uso de que el valor esperado del vector $\mathcal{Z}$ tiene que ser nulo. Y además $\mathcal{Z}\mathcal{Z}^T$ Es una matriz de covarianza que podemos definir a partir de la ecuación \ref{eq:cov} como $\sigma_zK$ con $K_{i,j} = \xi(x^{(i)},x^{(j)})$.  

\item \begin{equation}
\begin{split}
E[(r^T(x)\beta+Z(x))^2]=\\
r(x)^T\beta\beta^Tr(x) + 2r(x)^T\beta E[Z(x)] + E[Z(x)^2]=\\
= r^T\beta\beta^Tr + \sigma_z^2
\end{split}
\end{equation}
De nuevo, ecuación (\ref{eq:cov}),  $E[Z(x)] = 0$ y $\text{cov}(Z(x),Z(x)) = \sigma_z^2$

\item \begin{equation}
\begin{split}
E[-2a_x^T(R\beta + \mathcal{Z})(r^T(x)\beta +Z(x))]=\\
-2a_x^TR\beta r(x)^T\beta -2a_x^TR\beta E[Z(x)]-2a_x^TE[\mathcal{Z}]r(x)^T\beta + -2a_x^TE[\mathcal{Z}Z(x)]=\\
=-2a_x^TR\beta r(x)^T\beta -2a_x^T\sigma_zk_x
\end{split}
\end{equation}
Donde, $k_x = \sigma_z^2(\xi(x,x_1),\xi(x,x_2),\cdots,\xi(x,x_n))^T$ y las esperanzas de $Z$ y $\mathcal{Z}$ son cero.
\end{enumerate}
Llegamos por tanto a que,
\begin{equation}\label{eq:min}
\begin{split}
E[(a_x^Ty-Y(x))^2] =\\
a_x^TR\beta\beta^TR^Ta_x +a_x^T\sigma_z^2Ka_x+r(x)^T\beta\beta^Tr(x) + \sigma_z^2-2a_x^TR\beta r(x)^T\beta -2a_x^T\sigma_zk_x=\\
=(a_x^TR\beta-r(x)^T\beta)^2+a_x^T\sigma_z^2Ka_x+\sigma_z^2-2a_x^T\sigma_z^2 k_x
\end{split}
\end{equation}

A nosotros nos gustaría minimizar esta expresión, ya que eso es tanto como minimizar el error esperado. Si nos fijamos en el primer término cuadrático, si nuestro ajuste es bueno, deberíamos hacer este término cero: $R^Ta_x = r(x)$, Si lo hacemos así, estamos construyendo un estimador sin sesgo. $Y = a_x^Ty = a_x^TR\beta$ donde $Y$ es el valor de la variable estocástica que queremos predecir $Y = r(x)^T\beta$.

Por tanto deberíamos minimizar el resto de expresión,
\begin{equation}
\begin{split}
&\min_{a_x}\left[{a_x^T\sigma_z^2 Ka_x+\sigma_z^2-2a_x^T\sigma_z^2 k_x}\right]\\
&\text{sujeto a:}\\
& R^Ta_x = 0
\end{split}
\end{equation}

Si derivamos con respeto a $a^T$, igualamos a cero y añandimos un multiplicador de Lagranje $-\lambda R$ a la expresión derivada, obtenemos:
\begin{equation}
\begin{split}
&Ka_x - k_x-R\lambda = 0\\
&R^Ta_x = r(x)
\end{split}
\end{equation} 
Podemos rescribir las ecuaciones en forma matricial,
\begin{equation}
\begin{pmatrix}
K & R\\
R^T & 0
\end{pmatrix} \begin{pmatrix}
a_x \\ -\lambda
\end{pmatrix}= \begin{pmatrix}k_x \\ r(x)
\end{pmatrix}
\end{equation}

\begin{equation}
 \begin{pmatrix}
a_x \\ -\lambda
\end{pmatrix}= \begin{pmatrix}
K & R\\
R^T & 0
\end{pmatrix}^{-1}\begin{pmatrix} k_x \\ r(x)
\end{pmatrix}
\end{equation}

Podemos calcular la inversa de esta matriz, empleando los bonitos complementos de Shür y la eliminación de Gauss-Jordan por bloques\footnote{
https://en.wikipedia.org/wiki/Schur\_complement},

\begin{equation*}
M = \begin{bmatrix}
A & B\\
C & D
\end{bmatrix},\; M^{-1} = \begin{bmatrix}
A^{-1}+A^{-1}B(D-A^{-1}BC)^{-1}CA^{-1} & -A^{-1}B(D-A^{-1}BC)^{-1}\\
-(D-A^{-1}BC)^{-1}CA^{-1} & (D-A^{-1}BC)^{-1}
\end{bmatrix}
\end{equation*}

Si lo aplicamos a nuestro sistema,
\begin{equation}
 \begin{pmatrix}
a_x \\ -\lambda
\end{pmatrix}= \begin{pmatrix}
K^{-1}-K^{-1}R(K^{-1}RR^T)^{-1}R^TK^{-1} & K^{-1}R(K^{-1}RR^T)^{-1}\\
(K^{-1}RR^T)^{-1}R^TK^{-1} & -(K^{-1}RR^T)^{-1}
\end{pmatrix}^{-1}\begin{pmatrix} k_x \\ r(x)
\end{pmatrix}
\end{equation}

Con lo que optenemos para $a_x$,
\begin{equation}
a_x = (K^{-1}-K^{-1}R(K^{-1}RR^T)^{-1}R^TK^{-1})k_x +  K^{-1}R(K^{-1}RR^T)^{-1}r(x)
\end{equation}

Luego nuestra estimación del valor de $Y(x)$ sería,
\begin{equation}
\begin{split}
Y(x) = a_x^Ty =\\
\left[k_x^T(K^{-1}-K^{-1}R(R^TRK^{-1})^{-1}R^TK^{-1})+r(x)^T(R^TRK^{-1})^{-1}R^TK^{-1}\right]y=\\
k_x^TK^{-1}(y-R\beta)+r(x)^T\beta
\end{split}
\end{equation}
Que es precisamente la ecuación (18) del artículo con,
\begin{equation}
\beta = (R^TRK^{-1})^{-1}R^TK^{-1}
\end{equation}

En realidad, si somos rigurosos, el valor de $\beta$ que estamos empleando es una estimación, y deberiamos llamarle $\hat{\beta}$, siguiendo una notación que es más o menos estándar,para dejar claro que es un valor estimado.

Lo que hemos obtenido es el valor esperado para el modelo lineal que suponemos siguen nuestros predictores. Nos faltaría calcular la varianza. En realidad, coincide con el valor cuadrático medio que hemos optimizado,
\begin{equation}\label{eq:var}
\begin{split}
\sigma_{\phi}^2(x) = E[(a_x^Ty-Y(x))^2] =\\
E[(k_x^TK^{-1}(y-R\beta)+r(x)^T\beta-r(x)^T\beta-Z(x))^2] =\\
E[(k_x^TK^{-1}(y-R\beta)+Z(x))^2]=\\
E[(k_x^TK^{-1}(R\beta+\mathcal{Z}-R\beta)-Z(x))^2]=\\
E[(k_x^TK^{-1}\mathcal{Z}-Z(x))^2]
\end{split}
\end{equation}

Si desarrollamos el cuadrado de la última expresión de la ecuación (\ref{eq:var}). 

\begin{equation}
\begin{split}
\sigma_{\phi}^2(x) = E[(k_x^TK^{-1}\mathcal{Z}+Z(x))^2]=\\
E\left[k_x^TK^{-1}\mathcal{Z}\mathcal{Z}^TK^{-1}k_x -2k_x^TK^{-1}\mathcal{Z}Z(x)+ Z(x)^2\right]=\\
k_x^TK^{-1}E[\mathcal{Z}\mathcal{Z}^T]k_x -2k_x^TK^{-1}\sigma_zk_x+\sigma_z^2=\\
k_x^TK^{-1}\sigma_zKK^{-1}k_x - 2k_x^TK^{-1}\sigma_zk_x+\sigma_z^2=\\
= (k_x^TK^{-1}k_x -1)\sigma_z^2
\end{split}
\end{equation}

\end{document}